\documentclass{article}
%\usepackage{graphics}
\usepackage{fancybox}
%\usepackage{pspicture}  % More beutiful
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{geometry}

\newcommand{\forvec}[1]{\overrightarrow{\emph{#1}}}
\newcommand{\backvec}[1]{\overleftarrow{\emph{#1}}^{^{18}}}
\newcommand{\unionvec}[1]{\forvec{#1} \cup \backvec{#1}}
\newcommand{\deltahomeo}[2]{#1 \stackrel{\varepsilon}{\approx} #2}
\newcommand{\kdmer}[2]{
  (\ensuremath{#1 | #2})
}%

\begin{document}

\title{de Bruijn assembly using paired words}
\author{M. J. Chaisson \thanks{Department of Computer Science and
    Engineering, University of California San Diego, La Jolla, CA 92093-0404, USA.} 
\thanks{To whom correspondence should be addressed.}
\and P. A. Pevzner$^\ast$
\and G. P. Tesler
\thanks{Department of Mathematics, University of California, San Diego,
La Jolla, CA, 92093-0112, USA.}
}

\maketitle{}

\begin{abstract}
  To be written after writing the paper.
\end{abstract}

\section{Introduction}
  
In \textit{de novo} fragment assembly, one aims to reconstruct the
entire sequence of a genome using sequenced overlapping fragments of
the genome.  Under the assumption of parsimony and in the ideal case
of sequencing without errors, the correct assembly of the genome is
the shortest assembly that contains all fragments as a substring.
This is computationally formulated as the Shortest Common Superstring
(SCS) problem.  The SCS problem is known be NP-Hard
~\cite{Storer1980}, so unfortunately it is impossible to solve exactly
in a resonable amount of time.  In such cases in Computational Biology
it is important to either define what are the theoretical
computational limitations, or provide new data for the problem that
allows a formulation which is computationally feasible.  When the new
data is more costly to produce it may not be clearly advantageous to
use.  A prominent example of this was the debate over BAC-by-BAC
mversus whole genome shotgun sequencing of the Human genome
~\cite{Green1997, Myers1997}.  In BAC-by-BAC sequencing, the genome
was physically separated into small 100Kb bacterial artificial
chromosomes, which were then easily assembled compared to whole-genome
shotgun sequencing, where reads sampled from the entire genome were
sequenced at once.  The production of a library of BACs is a laborious
and expensive task, and to date has only been used for the human and
mouse genomes.  The analysis of feasibility of assembly of the human
genome using the Whole-Genome Shotgun method was done in the abscence
of a reference genome through extensive simulations~\cite{Myers1997},
and was later shown using real data~\cite{HumanGenomeVenter}.

A similar discussion exists for the application of massively parallel
sequencing to \textit{de novo} assembly.  Methods for massively
parallel sequencing have a throughput of 500M bases (bp)-2Gbp per day by
reading the sequences of millions of templates at a
time~\cite{Shendure2008}.  The high throughput is achieved in some
cases at the cost of shorter read length and higher error rate
relative to Sanger sequencing~\cite{Pettersson2009}.  The throughput
and accuracy statistics for the three main sequencing platforms:
Applied Biosciences SOLiD, 454 Life Sciences, and the Illumina Genome
Analyzer are shown in Figure~\ref{tab:PlatformStatistics}.  Since many
platforms trade read length for sequencing speed, the question arises
whether shorter reads are appropriate for \textit{de novo} assembly.
Initial studies for the feasibility of \textit{de novo} assembly of
short, under 100 base reads, indicated that these reads when unpaired
could only support very fragmented assemblies of mammalian
genomes~\cite{Chaisson2004, Whiteford2005} because the short reads are
unable to resolve the complicated repeat structures common to these
genomes.  Even with the release of paired-end information for short
reads, \textit{de novo} assembly of short reads has been limited to
bacterial genomes.

Recently, an assembly of the Human genome using 35 base, paired end
Illumina reads was published~\cite{Birol2009}.  While many single
nucleotide and small structural polymorphisms were detected, the
average contig size was 0.79 Kb and total assembly 2.18
Gbp~\cite{Birol2009}, a decrease in assembly quality compared to the
original whole-genome shotgun assembly of the human genome, with an
average contig size of 11.7Kb covering 2.59 Gbp of the genome, with
lower read coverage~\cite{Venter2001} 500-750bp Sanger reads.  This
raises the question: are the longer read lengths required for higher
assembly qualities on mammalian genomes?  The studies performed
in~\cite{Chaisson2004,Whiteford2005} on assembly with short unpaired
reads rely the theory developed for Sequencing by Hybridization
CITE:Pevzner1989, that the optimal assembly for reads of length $k$ is
given by the de Bruijn graph using vertices of size $k-1$, assuming a
read is sampled at every position in the genome.  Long ago it was
reported that the additional information contained by sequencing from
opposite ends of a clone adds considerable information to an
assembly~\cite{Caskey1991}, and so it may be possible for read-pairs
to provide the same amount of information as unpaired longer reads.
In \cite{Pevzner2001db}, it was shown that one may use a de Bruijn
graph to transform paired reads: ($read_1, \textrm{gap of length\ } d,
read_2$) into a mate-read: ($read_1 - \textrm{sequence of length} d -
read_2$), effectively transforming a pair of reads to one long read of
length len(read$_1$) + len(read$_2$) + $d$.  Depending on the
complexity of the graph, not all read-pairs may be transformed into
mate-reads.  The complexity of a de Bruijn graph decreases as the size
of the $k$-mer used for every vertex increases, and one may use larger
values of $k$ with larger read lengths, implying that longer reads
allow greater transformation from paired-reads into mate-reads.
However, it was observed in~\cite{Chaisson2009} that the number of
paired reads that may be transformed into mate-reads increases with
read length, though in bacterial and yeast genomes there is a point at
which increasing the read length does not increase the number of
read-pairs that are transformed into mate-reads.







\begin{table}
\begin{tabular}{lrrr}
Company & Read length & Gb/day & Error rate \\
\hline
Applied Biosciences 3730   & .01 & .0001 & 1\% \\
454 Life Sciences   & 1000 & 1 & .1/1\%\footnote{Error rates for mutations/indels.} \\
Applied Biosciences & 50   & 5 & 3\% \\
Illumina            & 120  & 5 & 1\% 
\end{tabular}
\caption{Sequencing statistics of the industrial massively paralell
  sequencing platforms, based on company predictions for sequencing
  capability in early 2010.  NOTE DURING DRAFT: THESE NUMBERS ARE
  ESTIMATES AND SHOULD BE DOUBLE CHECKED WIHT PRESS RELEASES.  I
  propose replacing this chart with two y-axes, read length and base-throughput/time.}
\label{tab:PlatformStatistics}
\end{table}





\section{Methods}

\input{pedebruijn_methods}

\section{Results}

\input{pedebruijn_results}


\section{Conclusions}

\input{pedebruijn_conclusions}


\bibliographystyle{plain}
\bibliography{citations}


\end{document}
