configfile: "config.yaml"

import functools
import os
import os.path

#Config parameters
IN = config["IN"]
SPADES = config["SPADES"]
SPADES_REASSEMBLY = config.get("SPADES_REASSEMBLY", SPADES)
BIN = config["BIN"]
SCRIPTS = config["SCRIPTS"]
SOFT = config["SOFT"]
QUAST = config["QUAST"]
# TODO: there should be only one
METAQUAST = QUAST.replace("quast.py", "metaquast.py")
K = int(config.get("K", 55))
SMALL_K = int(config.get("SMALL_K", 21))
MIN_CONTIG_LENGTH = int(config["MIN_CONTIG_LENGTH"])
THREADS = config.get("THREADS", 16)

#Path to saves of necessary assembly stage
SAVES = "K{0}/saves/01_before_repeat_resolution/graph_pack".format(K)

#Flag files for stages completion
FINISHED_BINNING = "binning/all.log"
FINISHED_CHOOSING = "binning/choose_all.log"

#Autodetect samples and their reads
SAMPLES, = glob_wildcards(IN + "/{sample,sample\d+}")
SAMPLE_COUNT = len(SAMPLES)

FASTA_EXTS = {".fasta", ".fa", ".fna", ".fsa", ".fastq", ".fastq.gz", ".fq", ".fq.gz"}
def gather_paths(path, basename=False):
    for filename in os.listdir(path):
        name, ext = os.path.splitext(filename)
        if ext in FASTA_EXTS:
            filepath = os.path.join(path, filename)
            if basename:
                yield (name, filepath)
            else:
                yield filepath

def detect_reads(sample):
    path = "{}/{}/".format(IN, sample)
    return (sample, sorted(list(gather_paths(path)))[:2])

SAMPLE_READS = dict(map(detect_reads, SAMPLES))

def sample_reads(dir, wildcards):
    return SAMPLE_READS[wildcards.sample][dir]

left_reads  = functools.partial(sample_reads, 0)
right_reads = functools.partial(sample_reads, 1)

#Autodetect bins
CAGS, = glob_wildcards("binning/{cag,CAG\d+}")
GOOD_CAGS, = glob_wildcards("binning/{cag,CAG\d+}/left.fastq")

#Autodetect references

def gather_refs(data):
    if type(data) is list:
        for path in data:
            yield from gather_refs(path)
    else:
        if os.path.isdir(data):
            yield from gather_paths(data, True)
        else:
            yield (os.path.splitext(os.path.basename(data))[0], data)

REFS = dict(gather_refs(config.get("REFS", [])))
ALL_REFS = ",".join(path for path in REFS.values())

def ref_path(wildcards):
    return REFS[wildcards.ref]

onstart:
    for dir in ["annotation", "assembly", "profile",
                "reassembly", "stats", "stats/summary", "tmp"]:
        try:
            os.mkdir(dir)
        except:
            pass

# ---- Main pipeline -----------------------------------------------------------

rule all:
    input:   FINISHED_BINNING
    message: "Dataset of {SAMPLE_COUNT} samples from {IN} has been processed."

rule assemble:
    input:   left = left_reads, right = right_reads
    output:  "assembly/{sample,sample\d+}.fasta"
    params:  "assembly/{sample}"
    log:     "assembly/{sample}.log"
    threads: THREADS
    message: "Assembling {wildcards.sample} with SPAdes"
    shell:   "{SPADES}/spades.py --meta -m 400 -t {threads} -1 {input.left} -2 {input.right}"
             " -o {params} >{log} 2>&1 && "
             "cp {params}/scaffolds.fasta {output}"

rule assemble_all:
    input:   expand("assembly/{sample}.fasta", sample=SAMPLES)
    message: "Assembled all samples"

rule descriptions:
    output:  expand("profile/{sample}.desc", sample=SAMPLES)
    message: "Generating sample descriptions"
    run:
        for sample in SAMPLES:
            with open("profile/{}.desc".format(sample), "w") as out:
                wildcards.sample = sample
                print(left_reads(wildcards),  file=out)
                print(right_reads(wildcards), file=out)

rule kmc:
    input:   "profile/{sample}.desc"
    output:  temp("tmp/{sample}.kmc_pre"), temp("tmp/{sample}.kmc_suf")
    params:  min_mult=2, tmp="tmp/{sample}_kmc", out="tmp/{sample}"
    log:     "profile/kmc_{sample}.log"
    threads: THREADS
    message: "Running kmc for {wildcards.sample}"
    shell:   "mkdir {params.tmp} && "
             "{SOFT}/kmc -k{SMALL_K} -t{threads} -ci{params.min_mult} -cs65535"
             " @{input} {params.out} {params.tmp} >{log} 2>&1 && "
             "rm -rf {params.tmp}"

rule multiplicities:
    input:   expand("tmp/{sample}.kmc_pre", sample=SAMPLES), expand("tmp/{sample}.kmc_suf", sample=SAMPLES)
    output:  "profile/kmers.mpl"
    params:  kmc_files=" ".join(expand("tmp/{sample}", sample=SAMPLES)), out="profile/kmers"
    log:     "profile/kmers.log"
    message: "Gathering {SMALL_K}-mer multiplicities from all samples"
    shell:   "{BIN}/kmer_multiplicity_counter -k {SMALL_K} -o {params.out} --sample 3"
             " -f {params.kmc_files} >{log} 2>&1 && "
             "rm tmp/*.sorted"

rule profile:
    input:   contigs="assembly/{sample}.fasta", mpl="profile/kmers.mpl"
    output:  "profile/{sample}.id", "profile/{sample}.mpl"
    log:     "profile/{sample}.log"
    message: "Counting contig abundancies for {wildcards.sample}"
    shell:   "{BIN}/contig_abundance_counter -k {SMALL_K} -w tmp -c {input.contigs}"
             " -n {SAMPLE_COUNT} -m profile/kmers -o profile/{wildcards.sample}"
             " -f assembly/{wildcards.sample}_splits.fasta"
             " -l {MIN_CONTIG_LENGTH} >{log} 2>&1"

rule canopy_pre:
    input:   expand("profile/{sample}.id", sample=SAMPLES)
    output:  "profile/canopy.in"
    message: "Preparing canopy input"
    shell:   "{SCRIPTS}/make_canopy_input.py {output} {input}"

rule canopy:
    input:   rules.canopy_pre.output
    output:  out = "profile/canopy.out", prof = "profile/canopy.prof"
    message: "Running canopy clustering"
    shell:   "{SCRIPTS}/canopy_launch.sh {input} {output.out} {output.prof} >profile/canopy.log 2>&1"

rule canopy_post:
    input:   rules.canopy.output.out
    output:  expand("annotation/{sample}.ann", sample=SAMPLES)
    message: "Preparing raw annotations"
    shell:   "{SCRIPTS}/parse_canopy_out.py {input} ./annotation"

rule binning:
    input:   contigs="assembly/{sample}.fasta", ann="annotation/{sample}.ann",
             left=left_reads, right=right_reads
    output:  "annotation/{sample}_edges.ann"
    params:  saves="assembly/{sample}/" + SAVES, splits="assembly/{sample}_splits.fasta"
    log:     "binning/{sample}.log"
    message: "Propagating annotation & binning reads for {wildcards.sample}"
    shell:   "{BIN}/prop_binning -k {K} -s {params.saves} -c {input.contigs}"
             " -a {input.ann} -f {params.splits} -l {input.left} -r {input.right}"
             " -o binning -n {wildcards.sample} >{log} 2>&1"

#Mock rule just for simplifying M-to-N dependencies in the stage graph
rule binning_all:
    input:   expand("annotation/{sample}_edges.ann", sample=SAMPLES)
    output:  touch(FINISHED_BINNING)
    message: "Binned all samples"

rule choose_samples:
    input:   flag=FINISHED_BINNING, prof=rules.canopy.output.prof
    #It's impossible to output "binning/{cag}/left.fastq", "binning/{cag}/right.fastq"
    #because the CAG can be filtered out
    output:  touch("binning/{cag,CAG\d+}.log")
    params:  cag="{cag}", dir="binning/{cag}"
    log:     "binning/choose_{cag}.log"
    message: "Choosing samples for {params.cag}"
    shell:   "{SCRIPTS}/choose_samples.py {params.cag} {input.prof} {params.dir} >{output} 2>&1"

#Launch this AFTER 'all'
rule choose_all:
    input:   expand("binning/{cag}.log", cag=CAGS)
    output:  touch(FINISHED_CHOOSING)
    message: "Filtered all CAGs"

rule generate_yaml:
    input:   FINISHED_CHOOSING
    output:  "reassembly/{cag}.yaml"
    message: "Generated config file for reassembly"
    run:
        with open(output[0], "w") as outfile:
            yaml = {"k": SMALL_K, "sample_cnt": SAMPLE_COUNT,
                    "kmer_mult": rules.multiplicities.params.out,
                    "bin": wildcards.cag, "bin_prof": rules.canopy.output.prof}
            #TODO: some sort of PyYAML
            for key, value in yaml.items():
                print(key, ": ", value, sep="", file=outfile)

rule reassemble:
    input:   left="binning/{cag}/left.fastq", right="binning/{cag}/right.fastq",
             config="reassembly/{cag}.yaml"
    output:  "reassembly/{cag}.fasta"
    params:  "reassembly/reassembly_{cag}"
    log:     "reassembly/reassembly_{cag}.log"
    threads: THREADS
    message: "Reassembling reads for {wildcards.cag}"
    shell:   "{SPADES_REASSEMBLY}/spades.py --meta -t {threads}"
             " --pe1-1 {input.left} --pe1-2 {input.right} --pe1-ff"
             " -o {params} --series-analysis {input.config} >{log} 2>&1 && "
             "cp {params}/scaffolds.fasta {output}"

#Launch this AFTER 'choose_all'
rule reassemble_all:
    input:   expand("reassembly/{cag}.fasta", cag=GOOD_CAGS)
    message: "Reassembled reads for all bins"

#===============================================================================
#---- Statistics section -------------------------------------------------------
#===============================================================================

#---- Single alignments for samples per reference -------------------------------
#TODO: use alignments from meta version instead
rule quast_all_samples:
    input:   ref=ref_path, contigs=expand("assembly/{sample}.fasta", sample=SAMPLES)
    output:  "stats/summary/q_{ref}.tsv"
    params:  "stats/{ref}"
    log:     "stats/{ref}/quast.log"
    message: "Aligning all samples on {wildcards.ref}"
    shell:   "{QUAST} -R {input.ref} {input.contigs} -o {params} >/dev/null 2>&1 && "
             "cp {params}/report.tsv {output}"

#---- Contigs of interest ------------------------------------------------------
rule filter_all_samples:
    input:   "stats/summary/q_{ref}.tsv"
    output:  expand("stats/{{ref}}/{sample}.cont", sample=SAMPLES)
    message: "Filtering interesting long contigs from all samples for {wildcards.ref}"
    params:  "stats/{ref}/"
    shell:   "{SCRIPTS}/filter_nucmer.py {params} {MIN_CONTIG_LENGTH} 70"

#---- GF of combined sample ----------------------------------------------------
rule combine_filtered:
    input:   contigs=expand("assembly/{sample}.fasta", sample=SAMPLES),
             filters=expand("stats/{{ref}}/{sample}.cont", sample=SAMPLES)
    output:  "stats/{ref}/{sample}.fasta"
    message: "Gathering all interesting contigs for {wildcards.ref} into a single assembly"
    shell:   "{SCRIPTS}/filter_contigs.py {SAMPLE_COUNT} {output} {input.contigs} {input.filters}"

rule quast_combined:
    input:   ref=ref_path, contigs="summary/{ref}.fasta"
    output:  "stats/q_{ref}_all/report.tsv"
    params:  "stats/q_{ref}_all"
    log:     "stats/q_{ref}_all.log"
    threads: THREADS
    message: "Aligning combined sample on {wildcards.ref}"
    shell:   "{QUAST} -t {threads} -R {input.ref} {input.contigs} -o {params} >{log} 2>&1"

# Run this
rule quast_combined_all:
    input:   expand("stats/q_{ref}_all/report.tsv", ref=REFS)
    message: "Calculated QUAST metrics on all combined samples"


#---- Bins of interest ---------------------------------------------------------
rule int_bins:
    input:   "annotation/{sample}.ann", "stats/{ref}/{sample}.cont"
    output:  "stats/{ref}/{sample}.bin"
    message: "Filtering interesting bins for {wildcards.sample} aligned to {wildcards.ref}"
    shell:   "{SCRIPTS}/filter_bins.py {input} > {output}"

rule int_bins_all_samples:
    input:   expand("stats/{{ref}}/{sample}.bin", sample=SAMPLES)
    output:  "stats/{ref}/total.bin"
    message: "Gathering interesting bins for {wildcards.ref} from all samples"
    run:
        bins = set()
        for in_fn in input:
            with open(in_fn) as infile:
                for line in infile:
                    bins.add(line)
        with open(output[0], "w") as outfile:
            for bin in bins:
                print(bin, file=outfile)

# Run this
rule int_bins_all:
    input:   expand("stats/{ref}/total.bin", ref=REFS)
    message: "Gathered all interesting bins"

#---- GF per bin per reference --------------------------------------
PROP = {"prelim": ("_splits", ""), "prop": ("_edges", "_edges")}

#TODO: split into different directories per sample
rule split_bins:
    input:   lambda w: "assembly/{}{}.fasta".format(w.sample, PROP[w.prop][0]),
             lambda w: "annotation/{}{}.ann".format(w.sample, PROP[w.prop][1])
    output:  touch("binning/{prop}/{sample}.log")
    log:     "binning/{prop}/split_{sample}.log"
    params:  "binning/{prop}"
    message: "Splitting assembly of {wildcards.sample} between {wildcards.prop} bins"
    shell:   "mkdir -p {params} && rm -f {params}/{wildcards.sample}-*.fasta && "
             "{SCRIPTS}/split_bins.py {wildcards.sample} {input} {params} >{log}"

rule cat_binned_contigs:
    input:   expand("binning/{{prop}}/{sample}.log", sample=SAMPLES)
    output:  "binning/{prop}/{cag,CAG\d+}.fasta"
    params:  "`ls binning/{prop}/*-{cag}.fasta`"
    message: "Combine binned contigs ({wildcards.prop}) for {wildcards.cag}"
    shell:   "cat {params} > {output}"

#Two helpers for determining dependencies of QUAST targets.
#For split contigs and reassemblies, we need only corresponding FASTA.
#For combined contigs, we need to glue their split pieces first.
def stats_input(wildcards):
    if wildcards.stage == "reassembly":
        return expand("reassembly/{cag}.fasta", cag=GOOD_CAGS)
    w_bin, w_prop = wildcards.stage.split("_", 2)
    if w_bin == "split":
        return expand("binning/{prop}/{sample}.log", prop=w_prop, sample=SAMPLES)
    elif w_bin == "bin":
        return expand("binning/{prop}/{cag}.fasta", prop=w_prop, cag=CAGS)

def stats_data(wildcards):
    if wildcards.stage == "reassembly":
        return "`ls reassembly/CAG*.fasta`"
    w_bin, w_prop = wildcards.stage.split("_", 2)
    masks = {"bin": "CAG*", "split": "*-CAG*"}
    return "`ls binning/{}/{}.fasta`".format(w_prop, masks[w_bin])

rule quast_stats:
    input:   stats_input
    output:  "stats/summary/gf_{stage}.tsv"
    params:  data=stats_data, out="stats/q_{stage}"
    log:     "stats/q_{stage}.log"
    threads: THREADS
    message: "Aligning {wildcards.stage} assemblies on all references"
    shell:   "{METAQUAST} -t {threads} -R {ALL_REFS} {params.data} -o {params.out} >{log} 2>&1 && "
             "cp '{params.out}/summary/TSV/Genome_fraction_(%).tsv' {output}"

# Run this AFTER 'all'
rule stats_all:
    input:   expand("stats/summary/gf_{bin}_{prop}.tsv", bin=["bin"], prop=PROP)
    message: "Gathered some numbers, deal with them."

# Run this AFTER 'reassembly_all'
rule stats_reassembly:
    input:   "stats/summary/gf_reassembly.tsv"
    message: "Gathered even more numbers, deal with them."

# Don't run this
rule neat_table:
    input:   "stats/q_split_prelim.log"
    output:  "stats/summary/table.tsv"
    params:  dir="stats/q_split_prelim", refs=" ".join(REFS)
    message: "Formatting a human-readable table from results"
    shell:   "{SCRIPTS}/format_table.py {output} {params.dir} {params.refs}"

#---- Propagator statistics ----------------------------------------------------
rule prop_stats:
    input:   prelim="annotation/{sample}.ann", prop="annotation/{sample}_edges.ann",
             contigs="assembly/{sample}.fasta", edges="assembly/{sample}_edges.fasta",
             ref=REFS.values() #, bins="{sample}/{ref}.bin"
    output:  "stats/prop_{cag}/{sample}.tsv"
    log:     "stats/prop_{cag}/{sample}.log"
    message: "Calculating propagation statistics for {wildcards.sample}"
    shell:   "mkdir -p stats/prop_{wildcards.cag} && "
             "{BIN}/stats -k {K} -s {wildcards.sample}/assembly/{SAVES} -r {input.ref} -c {input.contigs} -a {input.prelim} -e {input.edges} -p {input.prop} -b {wildcards.cag} -o {output} >{log}"

# Run this
rule prop_stats_all:
    input:   expand("stats/prop_{cag}/{sample}.tsv", sample=SAMPLES, cag=GOOD_CAGS)
    message: "Calculated propagation statistics"

#---- PCA ----------------------------------------------------------------------
rule pca:
    input:   rules.canopy_pre.output, rules.canopy.output.out, "{sample}.cont"
    output:  "stats/{sample}.png"
    message: "Doing some visualization"
    shell:
        "Rscript {SCRIPTS}/pca.R {input} {output}"

rule combine_contigs:
    input:   expand("stats/{ref}/{sample}.cont", sample=SAMPLES, ref=REFS)
    output:  "stats/total.cont"
    message: "Combining interesting contigs for all samples/references"
    run:
        shell("rm -f {output}")
        for sample in SAMPLES:
            for ref in REFS:
                shell("awk '{{print $0 \"\t{ref}\"}}' stats/{ref}/{sample}.cont >> {output}")

# Run this
rule pca_total:
    input:   rules.canopy_pre.output, rules.canopy.output.out, "stats/total.cont"
    output:  "stats/summary/pca.png"
    shell:   "Rscript {SCRIPTS}/pca.R {input} {output}"
